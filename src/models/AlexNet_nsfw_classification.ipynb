{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Tyler Chase\n",
    "\n",
    "Date: 2017/05/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model NSFW Classification\n",
    "\n",
    "This code uses an AlexNet model to classify an image as not safe for work (nsfw) or safe for work (sfw). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from util import import_dataset\n",
    "from model import Model, lazy_property\n",
    "from config import ModelConfig, TrainConfig\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default to auto import packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (25450, 128, 128, 3)\n",
      "Train subreddit labels shape:  (25450,)\n",
      "Train nsfw labels shape:  (25450,)\n",
      "Validation data shape:  (3181, 128, 128, 3)\n",
      "Validation subreddit labels shape:  (3181,)\n",
      "Validation nsfw labels shape:  (3181,)\n",
      "Test data shape:  (3182, 128, 128, 3)\n",
      "Test subreddit labels shape:  (3182,)\n",
      "Test nsfw labels shape:  (3182,)\n"
     ]
    }
   ],
   "source": [
    "# Form training, developement, and testing data sets\n",
    "address = r'/home/tylerchase/CS-231N-Final-Project/data/fullData//'\n",
    "address = r'../../data/fullData//'\n",
    "file_names = {}\n",
    "file_names['images'] = 'full_data.npy'\n",
    "file_names['subs'] = 'full_subredditlabels'\n",
    "file_names['dict'] = 'full_subredditIndex'\n",
    "file_names['nsfw'] = 'full_nsfwlabels'\n",
    "data, dictionary = import_dataset(address, file_names)\n",
    "\n",
    "# Print the sizes as a sanity check\n",
    "print('Train data shape: ', data.X_train.shape)\n",
    "print('Train subreddit labels shape: ', data.y_train.shape)\n",
    "print('Train nsfw labels shape: ', data.y_train_2.shape)\n",
    "print('Validation data shape: ', data.X_val.shape)\n",
    "print('Validation subreddit labels shape: ', data.y_val.shape)\n",
    "print('Validation nsfw labels shape: ', data.y_val_2.shape)\n",
    "print('Test data shape: ', data.X_test.shape)\n",
    "print('Test subreddit labels shape: ', data.y_test.shape)\n",
    "print('Test nsfw labels shape: ', data.y_test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Subreddit Statistics of Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarthPorn Submissions:  1362\n",
      "SkyPorn Submissions:  1359\n",
      "spaceporn Submissions:  1307\n",
      "MilitaryPorn Submissions:  1331\n",
      "GunPorn Submissions:  1324\n",
      "carporn Submissions:  1320\n",
      "CityPorn Submissions:  1336\n",
      "ruralporn Submissions:  969\n",
      "ArchitecturePorn Submissions:  1278\n",
      "FoodPorn Submissions:  1364\n",
      "MoviePosterPorn Submissions:  1354\n",
      "ArtPorn Submissions:  1349\n",
      "RoomPorn Submissions:  1357\n",
      "creepy Submissions:  1306\n",
      "gonewild Submissions:  982\n",
      "PrettyGirls Submissions:  1329\n",
      "ladybonersgw Submissions:  917\n",
      "LadyBoners Submissions:  1191\n",
      "cats Submissions:  1356\n",
      "dogpictures Submissions:  1359\n",
      "Sanity Check Sum:  25450\n",
      "\n",
      "Total Submissions:  25450\n"
     ]
    }
   ],
   "source": [
    "# Print and store subreddits and subreddit totals\n",
    "num_subs = len(dictionary)\n",
    "classes = [\"\"] * num_subs\n",
    "stats = [0] * num_subs\n",
    "\n",
    "# Form Array of Subreddits\n",
    "for sub, ind in dictionary.items():\n",
    "    classes[ind] = sub\n",
    "\n",
    "# Form array of Subreddit statistics and print\n",
    "for i, j in enumerate(classes):\n",
    "    temp = np.sum(i == data.y_train)\n",
    "    stats[i] = temp\n",
    "    print(j + ' Submissions: ', temp)\n",
    "print('Sanity Check Sum: ', np.sum(stats))\n",
    "\n",
    "# Print total submissions\n",
    "total = np.shape(data.y_train)[0]\n",
    "print('\\nTotal Submissions: ', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine NSFW Statistics in Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSFW Submissions:  2164\n",
      "SFW Submissions:  23286\n",
      "Sanity Check Sum:  25450\n",
      "\n",
      "Total Submissions:  25450\n"
     ]
    }
   ],
   "source": [
    "dict_nsfw = {}\n",
    "dict_nsfw['NSFW'] = 1\n",
    "dict_nsfw['SFW'] = 0\n",
    "\n",
    "# Print and store NSFW and NSFW totals\n",
    "num_out = len(dict_nsfw)\n",
    "classes_nsfw = [\"\"] * num_out\n",
    "stats_nsfw = [0] * num_out\n",
    "for category, ind in dict_nsfw.items():\n",
    "    classes_nsfw[ind] = category\n",
    "    temp = np.sum(ind == data.y_train_2)\n",
    "    stats_nsfw[ind] = temp\n",
    "    print(category + ' Submissions: ', temp)\n",
    "print('Sanity Check Sum: ', np.sum(stats_nsfw))\n",
    "\n",
    "total_nsfw = np.shape(data.y_train_2)[0]\n",
    "print('\\nTotal Submissions: ', total_nsfw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine NSFW Images Per Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarthPorn :  1362\n",
      "NSFW:  0\n",
      "SFW:  1362\n",
      "\n",
      "SkyPorn :  1359\n",
      "NSFW:  0\n",
      "SFW:  1359\n",
      "\n",
      "spaceporn :  1307\n",
      "NSFW:  0\n",
      "SFW:  1307\n",
      "\n",
      "MilitaryPorn :  1331\n",
      "NSFW:  4\n",
      "SFW:  1327\n",
      "\n",
      "GunPorn :  1324\n",
      "NSFW:  1\n",
      "SFW:  1323\n",
      "\n",
      "carporn :  1320\n",
      "NSFW:  1\n",
      "SFW:  1319\n",
      "\n",
      "CityPorn :  1336\n",
      "NSFW:  0\n",
      "SFW:  1336\n",
      "\n",
      "ruralporn :  969\n",
      "NSFW:  0\n",
      "SFW:  969\n",
      "\n",
      "ArchitecturePorn :  1278\n",
      "NSFW:  0\n",
      "SFW:  1278\n",
      "\n",
      "FoodPorn :  1364\n",
      "NSFW:  0\n",
      "SFW:  1364\n",
      "\n",
      "MoviePosterPorn :  1354\n",
      "NSFW:  10\n",
      "SFW:  1344\n",
      "\n",
      "ArtPorn :  1349\n",
      "NSFW:  110\n",
      "SFW:  1239\n",
      "\n",
      "RoomPorn :  1357\n",
      "NSFW:  0\n",
      "SFW:  1357\n",
      "\n",
      "creepy :  1306\n",
      "NSFW:  98\n",
      "SFW:  1208\n",
      "\n",
      "gonewild :  982\n",
      "NSFW:  982\n",
      "SFW:  0\n",
      "\n",
      "PrettyGirls :  1329\n",
      "NSFW:  0\n",
      "SFW:  1329\n",
      "\n",
      "ladybonersgw :  917\n",
      "NSFW:  917\n",
      "SFW:  0\n",
      "\n",
      "LadyBoners :  1191\n",
      "NSFW:  39\n",
      "SFW:  1152\n",
      "\n",
      "cats :  1356\n",
      "NSFW:  1\n",
      "SFW:  1355\n",
      "\n",
      "dogpictures :  1359\n",
      "NSFW:  1\n",
      "SFW:  1358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nsfw_breakdown = {}\n",
    "\n",
    "# Store and print NSFW breakdown of each Subreddit\n",
    "for i,j in enumerate(classes):\n",
    "    nsfw_sub = {}\n",
    "    class_indices = np.argwhere(data.y_train == i)\n",
    "    nsfw_subset = data.y_train_2[class_indices]\n",
    "    nsfw_sub['nsfw'] = np.sum(nsfw_subset == 1)\n",
    "    nsfw_sub['sfw'] = np.sum(nsfw_subset == 0)\n",
    "    nsfw_breakdown[j] = nsfw_sub\n",
    "    print(j, ': ', nsfw_sub['nsfw'] + nsfw_sub['sfw'])\n",
    "    print('NSFW: ', nsfw_sub['nsfw'])\n",
    "    print('SFW: ', nsfw_sub['sfw'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the SFW/NSFW Data Content\n",
    "\n",
    "Since safe for work SFW content takes up approximately 90% of the data here we balance the data by only considering 4 subreddits that are pictures of people. r/gonewild and r/ladybonersgw both contain mostly nsfw content and are women and men respectively. r/prettygirls and r/ladyboners both contain mostly sfw content and are women and men respectively.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gonewild\n",
      "posts found:  982\n",
      "\n",
      "ladybonersgw\n",
      "posts found:  917\n",
      "\n",
      "PrettyGirls\n",
      "posts found:  1329\n",
      "\n",
      "LadyBoners\n",
      "posts found:  1191\n",
      "\n",
      "sanity check\n",
      "posts found:  4419\n",
      "length training:  4419\n"
     ]
    }
   ],
   "source": [
    "subreddits_of_interest = ['gonewild', 'ladybonersgw', 'PrettyGirls', 'LadyBoners']\n",
    "total = 0\n",
    "for j,i in enumerate(subreddits_of_interest):\n",
    "    if j==0:\n",
    "        print(i)\n",
    "        index = dictionary[i] == data.y_train\n",
    "        found = np.sum(index)\n",
    "        print('posts found: ', found)\n",
    "        print()\n",
    "        total+=found\n",
    "        data_subset = data.X_train[index]\n",
    "        out_subset = data.y_train[index]\n",
    "        out_subset_2 = data.y_train_2[index]\n",
    "        data_subset_val = data.X_val[index]\n",
    "        out_subset_val = data.y_val[index]\n",
    "        out_subset_2_val = data.y_val_2[index]\n",
    "        data_subset_test = data.X_test[index]\n",
    "        out_subset_test = data.y_test[index]\n",
    "        out_subset_2_test = data.y_test_2[index]\n",
    "    else:\n",
    "        print(i)\n",
    "        index = dictionary[i] == data.y_train\n",
    "        found = np.sum(index)\n",
    "        print('posts found: ', found)\n",
    "        print()\n",
    "        total+=found\n",
    "        data_subset = np.concatenate((data_subset, data.X_train[index]), axis = 0)\n",
    "        out_subset = np.concatenate((out_subset, data.y_train[index]), axis = 0)\n",
    "        out_subset_2 = np.concatenate((out_subset_2, data.y_train_2[index]), axis = 0)\n",
    "        data_subset_val = np.concatenate((data_subset_val, data.X_val[index]), axis = 0)\n",
    "        out_subset_val = np.concatenate((out_subset_val, data.y_val[index]), axis = 0)\n",
    "        out_subset_2_val = np.concatenate((out_subset_2_val, data.y_val_2[index]), axis = 0)\n",
    "        data_subset_test = np.concatenate((data_subset_test, data.X_test[index]), axis = 0)\n",
    "        out_subset_test = np.concatenate((out_subset_test, data.y_test[index]), axis = 0)\n",
    "        out_subset_2_test = np.concatenate((out_subset_2_test, data.y_test_2[index]), axis = 0)\n",
    "        \n",
    "print('sanity check')\n",
    "print('posts found: ', total)\n",
    "print('length training: ', np.shape(data_subset)[0])\n",
    "        \n",
    "# Permute the training data for training \n",
    "SEED = 455\n",
    "random.seed(SEED)\n",
    "N = np.shape(out_subset)[0]\n",
    "indices = np.arange(N)\n",
    "random.shuffle(indices)\n",
    "data.X_train = data_subset[indices]\n",
    "data.y_train = out_subset[indices]\n",
    "data.y_train_2 = out_subset_2[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Subreddit Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarthPorn Submissions:  0\n",
      "SkyPorn Submissions:  0\n",
      "spaceporn Submissions:  0\n",
      "MilitaryPorn Submissions:  0\n",
      "GunPorn Submissions:  0\n",
      "carporn Submissions:  0\n",
      "CityPorn Submissions:  0\n",
      "ruralporn Submissions:  0\n",
      "ArchitecturePorn Submissions:  0\n",
      "FoodPorn Submissions:  0\n",
      "MoviePosterPorn Submissions:  0\n",
      "ArtPorn Submissions:  0\n",
      "RoomPorn Submissions:  0\n",
      "creepy Submissions:  0\n",
      "gonewild Submissions:  982\n",
      "PrettyGirls Submissions:  1329\n",
      "ladybonersgw Submissions:  917\n",
      "LadyBoners Submissions:  1191\n",
      "cats Submissions:  0\n",
      "dogpictures Submissions:  0\n",
      "Sanity Check Sum:  4419\n",
      "\n",
      "Total Submissions:  4419\n"
     ]
    }
   ],
   "source": [
    "# Print and store subreddits and subreddit totals\n",
    "num_subs = len(dictionary)\n",
    "classes = [\"\"] * num_subs\n",
    "stats = [0] * num_subs\n",
    "\n",
    "# Form Array of Subreddits\n",
    "for sub, ind in dictionary.items():\n",
    "    classes[ind] = sub\n",
    "\n",
    "# Form array of Subreddit statistics and print\n",
    "for i, j in enumerate(classes):\n",
    "    temp = np.sum(i == data.y_train)\n",
    "    stats[i] = temp\n",
    "    print(j + ' Submissions: ', temp)\n",
    "print('Sanity Check Sum: ', np.sum(stats))\n",
    "\n",
    "# Print total submissions\n",
    "total = np.shape(data.y_train)[0]\n",
    "print('\\nTotal Submissions: ', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the NSF Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSFW Submissions:  1938\n",
      "SFW Submissions:  2481\n",
      "Sanity Check Sum:  4419\n",
      "\n",
      "Total Submissions:  4419\n"
     ]
    }
   ],
   "source": [
    "dict_nsfw = {}\n",
    "dict_nsfw['NSFW'] = 1\n",
    "dict_nsfw['SFW'] = 0\n",
    "\n",
    "# Print and store NSFW and NSFW totals\n",
    "num_out = len(dict_nsfw)\n",
    "classes_nsfw = [\"\"] * num_out\n",
    "stats_nsfw = [0] * num_out\n",
    "for category, ind in dict_nsfw.items():\n",
    "    classes_nsfw[ind] = category\n",
    "    temp = np.sum(ind == data.y_train_2)\n",
    "    stats_nsfw[ind] = temp\n",
    "    print(category + ' Submissions: ', temp)\n",
    "print('Sanity Check Sum: ', np.sum(stats_nsfw))\n",
    "\n",
    "total_nsfw = np.shape(data.y_train_2)[0]\n",
    "print('\\nTotal Submissions: ', total_nsfw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine NSFW Images Per Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarthPorn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "SkyPorn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "spaceporn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "MilitaryPorn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "GunPorn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "carporn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "CityPorn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "ruralporn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "ArchitecturePorn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "FoodPorn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "MoviePosterPorn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "ArtPorn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "RoomPorn :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "creepy :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "gonewild :  982\n",
      "NSFW:  982\n",
      "SFW:  0\n",
      "\n",
      "PrettyGirls :  1329\n",
      "NSFW:  0\n",
      "SFW:  1329\n",
      "\n",
      "ladybonersgw :  917\n",
      "NSFW:  917\n",
      "SFW:  0\n",
      "\n",
      "LadyBoners :  1191\n",
      "NSFW:  39\n",
      "SFW:  1152\n",
      "\n",
      "cats :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n",
      "dogpictures :  0\n",
      "NSFW:  0\n",
      "SFW:  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nsfw_breakdown = {}\n",
    "\n",
    "# Store and print NSFW breakdown of each Subreddit\n",
    "for i,j in enumerate(classes):\n",
    "    nsfw_sub = {}\n",
    "    class_indices = np.argwhere(data.y_train == i)\n",
    "    nsfw_subset = data.y_train_2[class_indices]\n",
    "    nsfw_sub['nsfw'] = np.sum(nsfw_subset == 1)\n",
    "    nsfw_sub['sfw'] = np.sum(nsfw_subset == 0)\n",
    "    nsfw_breakdown[j] = nsfw_sub\n",
    "    print(j, ': ', nsfw_sub['nsfw'] + nsfw_sub['sfw'])\n",
    "    print('NSFW: ', nsfw_sub['nsfw'])\n",
    "    print('SFW: ', nsfw_sub['sfw'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define AlexNet model \n",
    "\n",
    "* 11x11 convolutional layer with 96 filters and a stride of 4\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "* batch normalization\n",
    "\n",
    "\n",
    "* 5x5 convolutional layer with 256 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "* batch normalization\n",
    "\n",
    "\n",
    "* 3x3 convolutional layer with 384 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 convolutional layer with 384 filters and a stride of 1\n",
    "* ReLU activation \n",
    "* 3x3 convolutional layer with 256 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "\n",
    "\n",
    "* affine layer from 4096 to 4096\n",
    "* ReLU activation\n",
    "* affine layer from 4096 to 4096\n",
    "* ReLU activation\n",
    "* affine layer from 4096 to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlexNet(Model):\n",
    "    \n",
    "    def __init__(self, model_config):\n",
    "        Model.__init__(self, model_config)\n",
    "  \n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # define our graph (e.g. AlexNet)\n",
    "        \n",
    "        a1 = tf.layers.conv2d(self.X_placeholder, filters=96, kernel_size=(11,11), strides=(4,4), padding='SAME') \n",
    "        h1 = tf.nn.relu(a1)\n",
    "        mp1 = tf.layers.max_pooling2d(h1, pool_size=(3,3), strides=(2,2), padding='SAME')    \n",
    "        bn1 = tf.layers.batch_normalization(mp1, training=self.is_training_placeholder)\n",
    "        \n",
    "        a2 = tf.layers.conv2d(bn1, filters=256, kernel_size=(5,5), strides=(1,1), padding='SAME')     \n",
    "        h2 = tf.nn.relu(a2)\n",
    "        mp2 = tf.layers.max_pooling2d(h2, pool_size=(3,3), strides=(2,2), padding='SAME')    \n",
    "        bn2 = tf.layers.batch_normalization(mp2, training=self.is_training_placeholder)              \n",
    "    \n",
    "        a3 = tf.layers.conv2d(bn2, filters=384, kernel_size=(3,3), strides=(1,1), padding='SAME')    \n",
    "        h3 = tf.nn.relu(a3)\n",
    "        a4 = tf.layers.conv2d(h3, filters=384, kernel_size=(3,3), strides=(1,1), padding='SAME')   \n",
    "        h4 = tf.nn.relu(a4)\n",
    "        a5 = tf.layers.conv2d(h4, filters=256, kernel_size=(3,3), strides=(1,1), padding='SAME')    \n",
    "        h5 = tf.nn.relu(a5)\n",
    "        mp3 = tf.layers.max_pooling2d(h5, pool_size=(3,3), strides=(2,2), padding='SAME')  \n",
    "    \n",
    "        mp_flat = tf.reshape(mp3,[-1,4096])\n",
    "        aff1 = tf.layers.dense(mp_flat, 4096)\n",
    "        h6 = tf.nn.relu(aff1)\n",
    "        aff2 = tf.layers.dense(h6, 4096)\n",
    "        h7 = tf.nn.relu(aff2)\n",
    "        y_out = tf.layers.dense(h7, self.config.class_size_2)\n",
    "    \n",
    "        return y_out     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Batch 1/45 of epoch 1 finished in 4.076715 seconds\n",
      "Epoch 1 training finished in 13.338714 seconds\n",
      "train accuracy:56.1%\n",
      "val accuracy:92.0%\n",
      "Epoch 1 evaluation finished in 15.673690 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 1/45 of epoch 2 finished in 0.192130 seconds\n",
      "Epoch 2 training finished in 8.586755 seconds\n",
      "train accuracy:56.1%\n",
      "val accuracy:92.0%\n",
      "Epoch 2 evaluation finished in 4.160239 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 1/45 of epoch 3 finished in 0.217906 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9ff3ff92d39d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Create session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/tylerchase/CS-231N-Final-Project/src/models/model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data, session, train_config)\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'improper output string use \"subreddit\" or \"nsfw\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 session.run(self.optimize, {self.X_placeholder:batch_X, \\\n\u001b[1;32m--> 108\u001b[1;33m                                             self.y_placeholder:batch_y,self.is_training_placeholder:True})\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[1;31m# print run time, current batch, and current epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create model instance\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model_config = ModelConfig(learning_rate=0.003, output = 'nsfw')\n",
    "train_config = TrainConfig(num_epochs=5, minibatch_size=100, print_every=100, \\\n",
    "    saver_address=r'../../subreddit_classification_parameters/', \\\n",
    "    save_file_name = 'AlexNet_nsfw_classification')\n",
    "model = AlexNet(model_config)\n",
    "\n",
    "# Create session\n",
    "session = tf.Session()\n",
    "model.train(data, session, train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return Loss and Accuracy History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Loss and Accuracy\n",
    "model.plot_loss_acc(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create model instance\n",
    "model_config = ModelConfig(learning_rate=0.003, output = 'nsfw')\n",
    "train_config = TrainConfig(num_epochs=2, minibatch_size=100, print_every=100, \\\n",
    "    saver_address=r'../../subreddit_classification_parameters/', \\\n",
    "    save_file_name = 'AlexNet_nsfw_classification')\n",
    "model = AlexNet(model_config)\n",
    "\n",
    "# Load Saved Model\n",
    "session = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, train_config.saver_address + train_config.save_file_name) \n",
    "\n",
    "# Test Model Accuracy\n",
    "loss_train, acc_train = model.eval(data, session, split='train')\n",
    "loss_val, acc_val = model.eval(data, session, split = 'val')\n",
    "\n",
    "print('Training Accuracy {:3.1f}%, Vallidation Accuracy:{:3.1f}%'.format((100*acc_train), (100*acc_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Predictions for Vallidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val_pred = session.run(model.prediction, {model.X_placeholder: data.X_val, model.y_placeholder: data.y_val_2, \n",
    "                                            model.is_training_placeholder:False})\n",
    "\n",
    "y_val_pred = np.argmax(y_val_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix for nsfw Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          save_address = ''):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j],2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    #plt.savefig(save_address + 'confusion_mat.png')\n",
    "\n",
    "classes = ['sfw', 'nsfw']\n",
    "\n",
    "conf = confusion_matrix(data.y_val_2, y_val_pred)\n",
    "plot_confusion_matrix(conf, classes=classes, normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
