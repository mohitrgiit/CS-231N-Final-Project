{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Tyler Chase\n",
    "\n",
    "Date: 2017/05/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model NSFW Classification\n",
    "\n",
    "This code uses an AlexNet model to classify an image as not safe for work (nsfw) or safe for work (sfw). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from util import import_dataset, sample_data, plot_confusion_matrix\n",
    "from model import Model, lazy_property\n",
    "from config import ModelConfig, TrainConfig\n",
    "from data_stats import DataStats\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default to auto import packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (25450, 128, 128, 3)\n",
      "Train subreddit labels shape:  (25450,)\n",
      "Train nsfw labels shape:  (25450,)\n",
      "Validation data shape:  (3181, 128, 128, 3)\n",
      "Validation subreddit labels shape:  (3181,)\n",
      "Validation nsfw labels shape:  (3181,)\n",
      "Test data shape:  (3182, 128, 128, 3)\n",
      "Test subreddit labels shape:  (3182,)\n",
      "Test nsfw labels shape:  (3182,)\n"
     ]
    }
   ],
   "source": [
    "# Form training, developement, and testing data sets\n",
    "address = r'/home/tylerchase/CS-231N-Final-Project/data/fullData//'\n",
    "address = r'../../data/fullData//'\n",
    "file_names = {}\n",
    "file_names['images'] = 'full_data.npy'\n",
    "file_names['subs'] = 'full_subredditlabels'\n",
    "file_names['dict'] = 'full_subredditIndex'\n",
    "file_names['nsfw'] = 'full_nsfwlabels'\n",
    "data, dictionary = import_dataset(address, file_names)\n",
    "\n",
    "# Print the sizes as a sanity check\n",
    "print('Train data shape: ', data.X_train.shape)\n",
    "print('Train subreddit labels shape: ', data.y_train.shape)\n",
    "print('Train nsfw labels shape: ', data.y_train_2.shape)\n",
    "print('Validation data shape: ', data.X_val.shape)\n",
    "print('Validation subreddit labels shape: ', data.y_val.shape)\n",
    "print('Validation nsfw labels shape: ', data.y_val_2.shape)\n",
    "print('Test data shape: ', data.X_test.shape)\n",
    "print('Test subreddit labels shape: ', data.y_test.shape)\n",
    "print('Test nsfw labels shape: ', data.y_test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Subreddit Statistics of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats = DataStats(data, dictionary)\n",
    "data_stats.sub_stats(dataSet = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats.sub_stats(dataSet = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine NSFW Statistics of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats.nsfw_stats(dataSet = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats.nsfw_stats(dataSet = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine NSFW Images Per Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats.subreddit_nsfw_stats(dataSet = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats.subreddit_nsfw_stats(dataSet = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Cell to take particular Subreddits from Dataset\n",
    "\n",
    "Since safe for work SFW content takes up approximately 90% of the data here we balance the data by only considering 4 subreddits that are pictures of people. r/gonewild and r/ladybonersgw both contain mostly nsfw content and are women and men respectively. r/prettygirls and r/ladyboners both contain mostly sfw content and are women and men respectively.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subreddits = ['gonewild', 'ladybonersgw', 'PrettyGirls', 'LadyBoners']\n",
    "dictionary_2 = sample_data(subreddits, data, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Subreddit Statistics of the Data Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats = DataStats(data, dictionary_2)\n",
    "data_stats.sub_stats(dataSet = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats.sub_stats(dataSet = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the NSF Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats.nsfw_stats(dataSet = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats.nsfw_stats(dataSet = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine NSFW Images Per Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats.subreddit_nsfw_stats(dataSet = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stats.subreddit_nsfw_stats(dataSet = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define AlexNet model \n",
    "\n",
    "* 11x11 convolutional layer with 96 filters and a stride of 4\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "* batch normalization\n",
    "\n",
    "\n",
    "* 5x5 convolutional layer with 256 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "* batch normalization\n",
    "\n",
    "\n",
    "* 3x3 convolutional layer with 384 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 convolutional layer with 384 filters and a stride of 1\n",
    "* ReLU activation \n",
    "* 3x3 convolutional layer with 256 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "\n",
    "\n",
    "* affine layer from 4096 to 4096\n",
    "* ReLU activation\n",
    "* affine layer from 4096 to 4096\n",
    "* ReLU activation\n",
    "* affine layer from 4096 to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlexNet(Model):\n",
    "    \n",
    "    def __init__(self, model_config):\n",
    "        Model.__init__(self, model_config)\n",
    "  \n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # define our graph (e.g. AlexNet)\n",
    "        \n",
    "        a1 = tf.layers.conv2d(self.X_placeholder, filters=96, kernel_size=(11,11), strides=(4,4), padding='SAME') \n",
    "        h1 = tf.nn.relu(a1)\n",
    "        mp1 = tf.layers.max_pooling2d(h1, pool_size=(3,3), strides=(2,2), padding='SAME')    \n",
    "        bn1 = tf.layers.batch_normalization(mp1, training=self.is_training_placeholder)\n",
    "        \n",
    "        a2 = tf.layers.conv2d(bn1, filters=256, kernel_size=(5,5), strides=(1,1), padding='SAME')     \n",
    "        h2 = tf.nn.relu(a2)\n",
    "        mp2 = tf.layers.max_pooling2d(h2, pool_size=(3,3), strides=(2,2), padding='SAME')    \n",
    "        bn2 = tf.layers.batch_normalization(mp2, training=self.is_training_placeholder)              \n",
    "    \n",
    "        a3 = tf.layers.conv2d(bn2, filters=384, kernel_size=(3,3), strides=(1,1), padding='SAME')    \n",
    "        h3 = tf.nn.relu(a3)\n",
    "        a4 = tf.layers.conv2d(h3, filters=384, kernel_size=(3,3), strides=(1,1), padding='SAME')   \n",
    "        h4 = tf.nn.relu(a4)\n",
    "        a5 = tf.layers.conv2d(h4, filters=256, kernel_size=(3,3), strides=(1,1), padding='SAME')    \n",
    "        h5 = tf.nn.relu(a5)\n",
    "        mp3 = tf.layers.max_pooling2d(h5, pool_size=(3,3), strides=(2,2), padding='SAME')  \n",
    "    \n",
    "        mp_flat = tf.reshape(mp3,[-1,4096])\n",
    "        aff1 = tf.layers.dense(mp_flat, 4096)\n",
    "        h6 = tf.nn.relu(aff1)\n",
    "        aff2 = tf.layers.dense(h6, 4096)\n",
    "        h7 = tf.nn.relu(aff2)\n",
    "        y_out = tf.layers.dense(h7, self.config.nsfw_class_size)\n",
    "    \n",
    "        return y_out     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 1 finished in 22.071638 seconds\n",
      "Batch 200/255 of epoch 1 finished in 18.504949 seconds\n",
      "Epoch 1 training finished in 52.399749 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 1 evaluation finished in 28.353721 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 2 finished in 18.724201 seconds\n",
      "Batch 200/255 of epoch 2 finished in 18.670653 seconds\n",
      "Epoch 2 training finished in 47.592092 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 2 evaluation finished in 14.938459 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 3 finished in 18.810408 seconds\n",
      "Batch 200/255 of epoch 3 finished in 18.675644 seconds\n",
      "Epoch 3 training finished in 47.695343 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 3 evaluation finished in 14.906647 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 4 finished in 18.735884 seconds\n",
      "Batch 200/255 of epoch 4 finished in 18.757121 seconds\n",
      "Epoch 4 training finished in 47.686278 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 4 evaluation finished in 14.886255 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 5 finished in 18.702726 seconds\n",
      "Batch 200/255 of epoch 5 finished in 18.671915 seconds\n",
      "Epoch 5 training finished in 47.572654 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 5 evaluation finished in 15.003880 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 6 finished in 18.756510 seconds\n",
      "Batch 200/255 of epoch 6 finished in 18.678884 seconds\n",
      "Epoch 6 training finished in 47.634981 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 6 evaluation finished in 14.902124 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 7 finished in 18.721129 seconds\n",
      "Batch 200/255 of epoch 7 finished in 18.686381 seconds\n",
      "Epoch 7 training finished in 47.606730 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 7 evaluation finished in 14.927538 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 8 finished in 18.722211 seconds\n",
      "Batch 200/255 of epoch 8 finished in 18.676216 seconds\n",
      "Epoch 8 training finished in 47.609749 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 8 evaluation finished in 14.938114 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 9 finished in 18.708653 seconds\n",
      "Batch 200/255 of epoch 9 finished in 18.669639 seconds\n",
      "Epoch 9 training finished in 47.583800 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 9 evaluation finished in 14.897780 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 10 finished in 18.767883 seconds\n",
      "Batch 200/255 of epoch 10 finished in 18.669604 seconds\n",
      "Epoch 10 training finished in 47.632483 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 10 evaluation finished in 14.800909 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 11 finished in 18.777423 seconds\n",
      "Batch 200/255 of epoch 11 finished in 18.663717 seconds\n",
      "Epoch 11 training finished in 47.647243 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 11 evaluation finished in 14.950346 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 12 finished in 18.731379 seconds\n",
      "Batch 200/255 of epoch 12 finished in 18.666292 seconds\n",
      "Epoch 12 training finished in 47.588558 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 12 evaluation finished in 14.943865 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 13 finished in 18.751175 seconds\n",
      "Batch 200/255 of epoch 13 finished in 18.666151 seconds\n",
      "Epoch 13 training finished in 47.607311 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 13 evaluation finished in 14.811748 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 14 finished in 18.759104 seconds\n",
      "Batch 200/255 of epoch 14 finished in 18.687242 seconds\n",
      "Epoch 14 training finished in 47.624794 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 14 evaluation finished in 14.812611 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 15 finished in 18.703718 seconds\n",
      "Batch 200/255 of epoch 15 finished in 18.714424 seconds\n",
      "Epoch 15 training finished in 47.599319 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 15 evaluation finished in 14.931527 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 16 finished in 18.781675 seconds\n",
      "Batch 200/255 of epoch 16 finished in 18.654802 seconds\n",
      "Epoch 16 training finished in 47.623849 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 16 evaluation finished in 14.833775 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 17 finished in 18.756151 seconds\n",
      "Batch 200/255 of epoch 17 finished in 18.685686 seconds\n",
      "Epoch 17 training finished in 47.624922 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 17 evaluation finished in 14.832917 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 18 finished in 18.663683 seconds\n",
      "Batch 200/255 of epoch 18 finished in 18.662980 seconds\n",
      "Epoch 18 training finished in 47.525301 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 18 evaluation finished in 14.930674 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 19 finished in 18.735479 seconds\n",
      "Batch 200/255 of epoch 19 finished in 18.677493 seconds\n",
      "Epoch 19 training finished in 47.596130 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 19 evaluation finished in 14.893796 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/255 of epoch 20 finished in 18.757611 seconds\n",
      "Batch 200/255 of epoch 20 finished in 18.665489 seconds\n",
      "Epoch 20 training finished in 47.618467 seconds\n",
      "train accuracy:91.5%\n",
      "val accuracy:92.0%\n",
      "Epoch 20 evaluation finished in 14.980506 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create model instance\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model_config = ModelConfig(learning_rate=0.003, output = 'nsfw')\n",
    "train_config = TrainConfig(num_epochs=20, train_batch_size=100, print_every=100, lr_decay=0.98,\\\n",
    "    saver_address=r'../../subreddit_classification_parameters/', \\\n",
    "    save_file_name = 'AlexNet_nsfw_classification')\n",
    "model = AlexNet(model_config)\n",
    "\n",
    "# Create session\n",
    "session = tf.Session()\n",
    "model.train(data, session, train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return Loss and Accuracy History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Loss and Accuracy\n",
    "model.plot_loss_acc(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create model instance\n",
    "model_config = ModelConfig(learning_rate=0.003, output = 'nsfw')\n",
    "train_config = TrainConfig(num_epochs=2, minibatch_size=100, print_every=100, \\\n",
    "    saver_address=r'../../subreddit_classification_parameters/', \\\n",
    "    save_file_name = 'AlexNet_nsfw_classification')\n",
    "model = AlexNet(model_config)\n",
    "\n",
    "# Load Saved Model\n",
    "session = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, train_config.saver_address + train_config.save_file_name) \n",
    "\n",
    "# Test Model Accuracy\n",
    "loss_train, acc_train = model.eval(data, session, split='train')\n",
    "loss_val, acc_val = model.eval(data, session, split = 'val')\n",
    "\n",
    "print('Training Accuracy {:3.1f}%, Vallidation Accuracy:{:3.1f}%'.format((100*acc_train), (100*acc_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Predictions for Vallidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val_pred = session.run(model.prediction, {model.X_placeholder: data.X_val, model.y_placeholder: data.y_val_2, \n",
    "                                            model.is_training_placeholder:False})\n",
    "\n",
    "y_val_pred = np.argmax(y_val_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix for nsfw Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = ['sfw', 'nsfw']\n",
    "\n",
    "conf = confusion_matrix(data.y_val_2, y_val_pred)\n",
    "plot_confusion_matrix(conf, classes=classes, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
