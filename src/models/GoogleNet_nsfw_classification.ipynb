{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import import_dataset\n",
    "\n",
    "address = '../../data/'\n",
    "file_names = {}\n",
    "file_names['images'] = 'full_data.npy'\n",
    "file_names['subs'] = 'full_subredditlabels'\n",
    "file_names['dict'] = 'full_subredditIndex'\n",
    "file_names['nsfw'] = 'full_nsfwlabels'\n",
    "data, dictionary = import_dataset(address, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25450, 128, 128, 3)\n",
      "(3181, 128, 128, 3)\n",
      "(3182, 128, 128, 3)\n",
      "(25450,)\n",
      "(3181,)\n",
      "(3182,)\n"
     ]
    }
   ],
   "source": [
    "print(data.X_train.shape)\n",
    "print(data.X_val.shape)\n",
    "print(data.X_test.shape)\n",
    "print(data.y_train.shape)\n",
    "print(data.y_val.shape)\n",
    "print(data.y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 1 finished in 19.121160 seconds\n",
      "Batch 200/398 of epoch 1 finished in 17.089528 seconds\n",
      "Batch 300/398 of epoch 1 finished in 17.118897 seconds\n",
      "Epoch 1 training finished in 70.726463 seconds\n",
      "train accuracy:89.8%\n",
      "val accuracy:89.5%\n",
      "Epoch 1 evaluation finished in 21.582336 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 2 finished in 17.122850 seconds\n",
      "Batch 200/398 of epoch 2 finished in 17.132186 seconds\n",
      "Batch 300/398 of epoch 2 finished in 17.114587 seconds\n",
      "Epoch 2 training finished in 68.096556 seconds\n",
      "train accuracy:94.1%\n",
      "val accuracy:93.8%\n",
      "Epoch 2 evaluation finished in 10.484437 seconds\n"
     ]
    }
   ],
   "source": [
    "from googlenet import GoogleNet\n",
    "from config import ModelConfig, TrainConfig\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model_config = ModelConfig(eval_batch_size=3000, learning_rate=5e-5, output=\"nsfw\")\n",
    "train_config = TrainConfig(print_every=100, num_epochs=2, saver_address=r'../../saved_params/', \\\n",
    "    save_file_name = 'GoogleNet_nsfw_classification_1e-3_96', lr_decay=0.96)\n",
    "model = GoogleNet(model_config)\n",
    "sess = tf.Session()\n",
    "model.train(data, sess, train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googlenet import GoogleNet\n",
    "from config import ModelConfig, TrainConfig\n",
    "import pickle\n",
    "\n",
    "# Reset Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create model instance\n",
    "model_config = ModelConfig()\n",
    "model = GoogleNet(model_config)\n",
    "\n",
    "# Load Saved Model\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "save_file = \"../../saved_params/GoogleNet_nsfw_classification\"\n",
    "saver.restore(sess, save_file) \n",
    "saved_history = pickle.load(open(save_file + \"_modelhist\", 'rb'))\n",
    "model.model_history = saved_history\n",
    "\n",
    "# Test Model Accuracy\n",
    "loss_train, acc_train = model.eval(data, sess, split='train')\n",
    "loss_val, acc_val = model.eval(data, sess, split = 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.plot_loss_acc(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from googlenet import GoogleNet\n",
    "from config import ModelConfig, TrainConfig\n",
    "import pickle\n",
    "import hyperopt as hpropt\n",
    "\n",
    "def objective(args):\n",
    "    model_config = ModelConfig(learning_rate=args['learning_rate'], keep_prob=args['keep_prob'], \\\n",
    "                               eval_batch_size=3000, output=\"nsfw\")\n",
    "    train_config = TrainConfig(num_epochs=args['num_epochs'], lr_decay=args['lr_decay'])\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    model = GoogleNet(model_config)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    model.train(data, sess, train_config)\n",
    "    cost, accuracy = model.eval(data, sess, \"val\")\n",
    "    \n",
    "    saveList = {\n",
    "        'accuracy' : accuracy,\n",
    "        'cost' : cost,\n",
    "        'num_epochs' : args['num_epochs'],\n",
    "        'learning_rate' : args['learning_rate'],\n",
    "        'lr_decay' : args['lr_decay'],\n",
    "        'keep_prob' : args['keep_prob']\n",
    "    }\n",
    "    print(saveList)\n",
    "    pickle.dump(saveList, open(\"../../hprOpt/nsfw_\" + str(accuracy) + \".dat\", \"wb\"))\n",
    "    model.plot_loss_acc(data)\n",
    "    return cost\n",
    "\n",
    "def optimize(space, max_evals=50):\n",
    "    best = hpropt.fmin(objective, space, algo=hpropt.tpe.suggest, max_evals=max_evals)\n",
    "    print(best)\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 1 finished in 18.164331 seconds\n",
      "Batch 200/398 of epoch 1 finished in 17.632418 seconds\n",
      "Batch 300/398 of epoch 1 finished in 17.634186 seconds\n",
      "Epoch 1 training finished in 70.583467 seconds\n",
      "train accuracy:92.5%\n",
      "val accuracy:93.0%\n",
      "Epoch 1 evaluation finished in 11.311978 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 2 finished in 17.559952 seconds\n",
      "Batch 200/398 of epoch 2 finished in 17.696228 seconds\n",
      "Batch 300/398 of epoch 2 finished in 17.729908 seconds\n",
      "Epoch 2 training finished in 70.193845 seconds\n",
      "train accuracy:93.4%\n",
      "val accuracy:93.7%\n",
      "Epoch 2 evaluation finished in 10.895666 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 3 finished in 17.591631 seconds\n",
      "Batch 200/398 of epoch 3 finished in 17.563115 seconds\n",
      "Batch 300/398 of epoch 3 finished in 17.568737 seconds\n",
      "Epoch 3 training finished in 69.927465 seconds\n",
      "train accuracy:94.3%\n",
      "val accuracy:94.6%\n",
      "Epoch 3 evaluation finished in 10.871137 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 4 finished in 17.581899 seconds\n",
      "Batch 200/398 of epoch 4 finished in 17.575976 seconds\n",
      "Batch 300/398 of epoch 4 finished in 17.631047 seconds\n",
      "Epoch 4 training finished in 69.972202 seconds\n",
      "train accuracy:94.6%\n",
      "val accuracy:94.6%\n",
      "Epoch 4 evaluation finished in 11.017680 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 5 finished in 17.578115 seconds\n",
      "Batch 200/398 of epoch 5 finished in 17.605474 seconds\n",
      "Batch 300/398 of epoch 5 finished in 17.588201 seconds\n",
      "Epoch 5 training finished in 69.941840 seconds\n",
      "train accuracy:95.1%\n",
      "val accuracy:94.8%\n",
      "Epoch 5 evaluation finished in 11.037503 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 6 finished in 17.578960 seconds\n",
      "Batch 200/398 of epoch 6 finished in 17.598044 seconds\n",
      "Batch 300/398 of epoch 6 finished in 17.588146 seconds\n",
      "Epoch 6 training finished in 69.961919 seconds\n",
      "train accuracy:95.4%\n",
      "val accuracy:94.8%\n",
      "Epoch 6 evaluation finished in 10.961187 seconds\n",
      "val accuracy:95.1%\n",
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 1 finished in 18.065508 seconds\n",
      "Batch 200/398 of epoch 1 finished in 17.459539 seconds\n",
      "Batch 300/398 of epoch 1 finished in 17.548462 seconds\n",
      "Epoch 1 training finished in 70.138500 seconds\n",
      "train accuracy:32.4%\n",
      "val accuracy:32.5%\n",
      "Epoch 1 evaluation finished in 11.110319 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 2 finished in 17.513116 seconds\n",
      "Batch 200/398 of epoch 2 finished in 17.466163 seconds\n",
      "Batch 300/398 of epoch 2 finished in 17.500068 seconds\n",
      "Epoch 2 training finished in 69.543622 seconds\n",
      "train accuracy:90.3%\n",
      "val accuracy:90.3%\n",
      "Epoch 2 evaluation finished in 11.065033 seconds\n",
      "---------------------------------------------------------\n",
      "Batch 100/398 of epoch 3 finished in 17.469345 seconds"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_learning_rate_range(base_10_min, base_10_max):\n",
    "    scale_factor = 1 / math.log10(math.e)\n",
    "    return hpropt.hp.loguniform('learning_rate', scale_factor * base_10_min, scale_factor * base_10_max)\n",
    "\n",
    "space = {\n",
    "        'num_epochs' : 6,\n",
    "        'learning_rate' : get_learning_rate_range(-6, -1.5),\n",
    "        'lr_decay' : hpropt.hp.uniform('lr_decay', 0.9, 1.0),\n",
    "        'keep_prob' : hpropt.hp.uniform('keep_prob', 0.6, 1.0)\n",
    "}\n",
    "\n",
    "best_opt = optimize(space, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get model predictions\n",
    "y_test_pred = sess.run(model.prediction, {model.X_placeholder: data.X_test, model.y_placeholder: data.y_test, \n",
    "                                            model.is_training_placeholder:False})\n",
    "\n",
    "y_test_pred = np.argmax(y_test_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "classes = [\"sfw\", \"nsfw\"]\n",
    "cm = confusion_matrix(data.y_test, y_test_pred)\n",
    "plot_confusion_matrix(cm, classes, normalize=True, task=\"nsfw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_score = get_f1_score(data.y_test, y_test_pred)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:94.3%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800.43861389160156, 0.94311762073368188)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval(data, sess, split=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
