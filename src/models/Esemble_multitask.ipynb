{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from config import ModelConfig, TrainConfig\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import import_dataset\n",
    "\n",
    "address = '../../data/'\n",
    "file_names = {}\n",
    "file_names['images'] = 'full_data.npy'\n",
    "file_names['subs'] = 'full_subredditlabels'\n",
    "file_names['dict'] = 'full_subredditIndex'\n",
    "file_names['nsfw'] = 'full_nsfwlabels'\n",
    "data, dictionary = import_dataset(address, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25450, 128, 128, 3)\n",
      "(3181, 128, 128, 3)\n",
      "(3182, 128, 128, 3)\n",
      "(25450,)\n",
      "(3181,)\n",
      "(3182,)\n"
     ]
    }
   ],
   "source": [
    "print(data.X_train.shape)\n",
    "print(data.X_val.shape)\n",
    "print(data.X_test.shape)\n",
    "print(data.y_train.shape)\n",
    "print(data.y_val.shape)\n",
    "print(data.y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit train accuracy:52.0%\n",
      "nsfw train accuracy:95.1%\n",
      "subreddit val accuracy:42.9%\n",
      "nsfw val accuracy:94.2%\n"
     ]
    }
   ],
   "source": [
    "from alexnet import AlexNetMulticlass\n",
    "\n",
    "# Reset Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create model instance\n",
    "model_config = ModelConfig(eval_batch_size=2000)\n",
    "model = AlexNetMulticlass(model_config)\n",
    "\n",
    "# Load Saved Model\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "save_file = \"../../saved_params/AlexNet_multitask_classification_postParamSearch\"\n",
    "saver.restore(sess, save_file) \n",
    "saved_history = pickle.load(open(save_file + \"_modelhist\", 'rb'))\n",
    "model.model_history = saved_history\n",
    "\n",
    "# Test Model Accuracy\n",
    "loss_train, acc_sbrd_train, acc_nsfw_train = model.eval(data, sess, split='train')\n",
    "loss_val, acc_sbrd_val, ac_nsfw_val = model.eval(data, sess, split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get model predictions\n",
    "alex_sbrd_logits, alex_nsfw_logits = sess.run(model.prediction, {model.X_placeholder: data.X_test, \\\n",
    "                                                               model.y_sbrd_placeholder: data.y_test, \\\n",
    "                                                               model.y_nsfw_placeholder: data.y_test_2, \\\n",
    "                                                               model.is_training_placeholder:False})\n",
    "alex_sbrd_pred = np.argmax(alex_sbrd_logits, axis = 1)\n",
    "alex_nsfw_pred = np.argmax(alex_nsfw_logits, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit train accuracy:93.6%\n",
      "nsfw train accuracy:98.7%\n",
      "subreddit val accuracy:64.1%\n",
      "nsfw val accuracy:96.7%\n"
     ]
    }
   ],
   "source": [
    "from googlenet import GoogleNetMulticlass\n",
    "\n",
    "# Reset Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create model instance\n",
    "model_config = ModelConfig(eval_batch_size=3000)\n",
    "model = GoogleNetMulticlass(model_config)\n",
    "\n",
    "# Load Saved Model\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "save_file = \"../../saved_params/GoogleNet_multitask_classification_4e-4_99\"\n",
    "saver.restore(sess, save_file) \n",
    "saved_history = pickle.load(open(save_file + \"_modelhist\", 'rb'))\n",
    "model.model_history = saved_history\n",
    "\n",
    "# Test Model Accuracy\n",
    "loss_train, acc_sbrd_train, acc_nsfw_train = model.eval(data, sess, split='train')\n",
    "loss_val, acc_sbrd_val, ac_nsfw_val = model.eval(data, sess, split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get model predictions\n",
    "goog_sbrd_logits, goog_nsfw_logits = sess.run(model.prediction, {model.X_placeholder: data.X_test, \\\n",
    "                                                               model.y_sbrd_placeholder: data.y_test, \\\n",
    "                                                               model.y_nsfw_placeholder: data.y_test_2, \\\n",
    "                                                               model.is_training_placeholder:False})\n",
    "goog_sbrd_pred = np.argmax(goog_sbrd_logits, axis = 1)\n",
    "goog_nsfw_pred = np.argmax(goog_nsfw_logits, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resnet import ResNetMulticlass\n",
    "\n",
    "# Reset Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create model instance\n",
    "model_config = ModelConfig(eval_batch_size=3000)\n",
    "model = ResNetMulticlass(model_config)\n",
    "\n",
    "# Load Saved Model\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "save_file = \"../../saved_params/ResNet_multitask_final\"\n",
    "saver.restore(sess, save_file) \n",
    "saved_history = pickle.load(open(save_file + \"_modelhist\", 'rb'))\n",
    "#model.model_history = saved_history\n",
    "\n",
    "# Test Model Accuracy\n",
    "loss_train, acc_sbrd_train, acc_nsfw_train = model.eval(data, sess, split='train')\n",
    "loss_val, acc_sbrd_val, ac_nsfw_val = model.eval(data, sess, split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get model predictions\n",
    "res_sbrd_logits, res_nsfw_logits = sess.run(model.prediction, {model.X_placeholder: data.X_test, \\\n",
    "                                                               model.y_sbrd_placeholder: data.y_test, \\\n",
    "                                                               model.y_nsfw_placeholder: data.y_test_2, \\\n",
    "                                                               model.is_training_placeholder:False})\n",
    "res_sbrd_pred = np.argmax(res_sbrd_logits, axis = 1)\n",
    "res_nsfw_pred = np.argmax(res_nsfw_logits, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this only when ResNet model can't be properly imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "res_sbrd_logits = pickle.load(open('../../test_sbrd_logits.dat', 'rb'))\n",
    "res_nsfw_logits = pickle.load(open('../../test_nsfw_logits.dat', 'rb'))\n",
    "res_sbrd_pred = pickle.load(open('../../test_sbrd_classes.dat', 'rb'))\n",
    "res_nsfw_pred = pickle.load(open('../../test_nsfw_classes.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions is a list of prediction arrays, one for each model\n",
    "# default_prediction is a prediction array, that is defaulted to when there is no clear majority\n",
    "# returns ensembled predictions\n",
    "def majority_vote_ensemble(predictions, default_prediction):\n",
    "    from scipy import stats\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    num_models = predictions.shape[1]\n",
    "    mode, counts = stats.mode(predictions)\n",
    "    counts = counts[0]\n",
    "    mode = mode[0]\n",
    "    indices = counts < (num_models / 2.0)  # indices of predictions with less than majority vote\n",
    "    vote_pred = mode\n",
    "    vote_pred[indices] = default_prediction[indices]\n",
    "    return vote_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used from CS 224N code\n",
    "def softmax(x):\n",
    "    shifted = x - np.max(x)\n",
    "    exponentiated = np.exp(shifted)\n",
    "    return exponentiated / np.sum(exponentiated)\n",
    "\n",
    "# predictions is a list of logit arrays, one for each model\n",
    "# weights is a list of weights for each model, in the same order as predictions\n",
    "# returns ensembled predictions\n",
    "def average_ensemble(logits, weights):\n",
    "    probs = np.array([softmax(x) for x in logits])\n",
    "    average_probs = np.average(probs, axis=0, weights=weights)\n",
    "    return np.argmax(average_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority vote accuracies:\n",
      "Subreddit: 0.494028912634\n",
      "NSFW: 0.944060339409\n",
      "Average accuracies:\n",
      "Subreddit: 0.473601508485\n",
      "NSFW: 0.949402891263\n"
     ]
    }
   ],
   "source": [
    "def accuracy(prediction, actual):\n",
    "    return np.average(prediction == actual)\n",
    "\n",
    "sbrd_logits = [alex_sbrd_logits, goog_sbrd_logits, res_sbrd_logits]\n",
    "nsfw_logits = [alex_nsfw_logits, goog_nsfw_logits, res_nsfw_logits]\n",
    "sbrd_preds = [alex_sbrd_pred, goog_sbrd_pred, res_sbrd_pred]\n",
    "nsfw_preds = [alex_nsfw_pred, goog_nsfw_pred, res_nsfw_pred]\n",
    "\n",
    "weights = [0.1, 0.7, 0.2]\n",
    "sbrd_majority_preds = majority_vote_ensemble(sbrd_preds, res_sbrd_pred)\n",
    "nsfw_majority_preds = majority_vote_ensemble(nsfw_preds, res_nsfw_pred)\n",
    "sbrd_average_preds = average_ensemble(sbrd_logits, weights)\n",
    "nsfw_average_preds = average_ensemble(nsfw_logits, weights)\n",
    "\n",
    "sbrd_majority_acc = accuracy(sbrd_majority_preds, data.y_test)\n",
    "nsfw_majority_acc = accuracy(nsfw_majority_preds, data.y_test_2)\n",
    "sbrd_average_acc = accuracy(sbrd_average_preds, data.y_test)\n",
    "nsfw_average_acc = accuracy(nsfw_average_preds, data.y_test_2)\n",
    "\n",
    "print(\"Majority vote accuracies:\")\n",
    "print(\"Subreddit: \" + str(sbrd_majority_acc))\n",
    "print(\"NSFW: \" + str(nsfw_majority_acc))\n",
    "print(\"Average accuracies:\")\n",
    "print(\"Subreddit: \" + str(sbrd_average_acc))\n",
    "print(\"NSFW: \" + str(nsfw_average_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
