{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Tyler Chase\n",
    "\n",
    "Date: 2017/05/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Classification\n",
    "\n",
    "This code uses a classification model to classify an image to one of 20 subreddits. For the sake of the milestone we implement an Alex Net model and analyze the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function for building graph\n",
    "\n",
    "This function reduces the text needed for running the @property wrapper making the code more readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduces the text needed for running @property making code more readable\n",
    "def lazy_property(function):\n",
    "    # Attribute used to test if code chunk has been run or not\n",
    "    attribute = '_lazy_' + function.__name__\n",
    "    # run wrapper function when wrapper returned below\n",
    "    @property\n",
    "    # Keeps original function attributes such as function.__name__ \n",
    "    # Otherwise it would be replaced with the wrapper attributes\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        # If doesn't have (attribute) then code chunk hasn't been run\n",
    "        if not hasattr(self, attribute):\n",
    "            # Run code chunk and store it in (attribute) of class\n",
    "            setattr(self, attribute, function(self))     \n",
    "        # return the value of the number stored in (attribute)\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import functools\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dbe5dc3d95fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nsfw'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'full_nsfwlabels'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dict'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'full_subredditIndex'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_val\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Print the sizes as a sanity check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-dbe5dc3d95fc>\u001b[0m in \u001b[0;36mimport_dataset\u001b[0;34m(address, file_names, train_percent, dev_percent)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimport_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mSEED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m455\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Read csv file and create a list of tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# Load data and split into training validation and testing sets\n",
    "\n",
    "# Function for permuting and splitting data into training, developement, and test\n",
    "def import_dataset(address, file_names, train_percent = 80, dev_percent = 10):\n",
    "    SEED = 455\n",
    "    random.seed(SEED)\n",
    "    # Read csv file and create a list of tuples\n",
    "    images = np.load(address+file_names['images'])\n",
    "    images = images.astype(float)\n",
    "    with open(address + file_names['subs'], 'rb') as file_2:\n",
    "        subs = pickle.load(file_2)\n",
    "        subs = np.array(subs)\n",
    "    with open(address + file_names['nsfw'], 'rb') as file_3:\n",
    "        nsfw = pickle.load(file_3)\n",
    "        nsfw = np.array(nsfw)\n",
    "    with open(address + file_names['dict'], 'rb') as file_4:\n",
    "        dictionary = pickle.load(file_4)\n",
    "    # Mix data and split into tran, dev, and test sets\n",
    "    N,W,H,C = np.shape(images)\n",
    "    indices = np.arange(N)\n",
    "    random.shuffle(indices)\n",
    "    images = images[indices]\n",
    "    subs = subs[indices]\n",
    "    nsfw = nsfw[indices]\n",
    "    train_end = int(train_percent*N/100)\n",
    "    dev_end = train_end + int(dev_percent*N/100)\n",
    "    X_train = images[:train_end]\n",
    "    y_train = subs[:train_end]\n",
    "    y2_train = nsfw[:train_end]\n",
    "    X_val = images[train_end:dev_end]\n",
    "    y_val = subs[train_end:dev_end]\n",
    "    y2_val = nsfw[train_end:dev_end]\n",
    "    X_test = images[dev_end:]\n",
    "    y_test = subs[dev_end:]\n",
    "    y2_test = nsfw[dev_end:]\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    return X_train, y_train, y2_train, X_val, y_val, y2_val, X_test, y_test, y2_test, dictionary\n",
    "\n",
    "# Form training, developement, and testing data sets\n",
    "address = r'/home/tylerchase/CS-231N-Final-Project/data/fullData//'\n",
    "file_names = {}\n",
    "file_names['images'] = 'full_data.npy'\n",
    "file_names['subs'] = 'full_subredditlabels'\n",
    "file_names['nsfw'] = 'full_nsfwlabels'\n",
    "file_names['dict'] = 'full_subredditIndex'\n",
    "X_train, y_train, y2_train, X_val, y_val, y2_val, \\\n",
    "    X_test, y_test, y2_test, dictionary = import_dataset(address, file_names)\n",
    "\n",
    "# Print the sizes as a sanity check\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train subreddit labels shape: ', y_train.shape)\n",
    "print('Train nsfw labels shape: ', y2_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation subreddit labels shape: ', y_val.shape)\n",
    "print('Validation nsfw labels shape: ', y2_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test subreddit labels shape: ', y_test.shape)\n",
    "print('Test nsfw labels shape: ', y2_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define AlexNet model \n",
    "\n",
    "* 11x11 convolutional layer with 96 filters and a stride of 4\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "* batch normalization\n",
    "\n",
    "\n",
    "* 5x5 convolutional layer with 256 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "* batch normalization\n",
    "\n",
    "\n",
    "* 3x3 convolutional layer with 384 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 convolutional layer with 384 filters and a stride of 1\n",
    "* ReLU activation \n",
    "* 3x3 convolutional layer with 256 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "\n",
    "\n",
    "* affine layer from 4096 to 1792\n",
    "* ReLU activation\n",
    "* affine layer from 1792 to 1792\n",
    "* ReLU activation\n",
    "* affine layer from 1792 to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration file storing important values\n",
    "class Config:\n",
    "    def __init__(self, sample_size, class_size, image_width, image_height, image_depth):\n",
    "        self.sample_size = sample_size\n",
    "        self.class_size = class_size\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "        self.image_depth = image_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlexNet:\n",
    "    \n",
    "    def __init__(self, config, learning_rate=0.003):\n",
    "        self.config = config\n",
    "        self.data, self.target, self.is_training = self._initialize_placeholders()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.prediction\n",
    "        self.cost\n",
    "        self.optimize\n",
    "        self.accuracy\n",
    "        \n",
    "    def _initialize_placeholders(self):\n",
    "        data = tf.placeholder(tf.float32, [None, self.config.image_height, \n",
    "                                         self.config.image_width, self.config.image_depth])\n",
    "        target = tf.placeholder(tf.int64, [None])\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "        return(data, target, is_training)\n",
    "        \n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # define our graph (e.g. AlexNet)\n",
    "        \n",
    "        a1 = tf.layers.conv2d(self.data, filters=96, kernel_size=(11,11), strides=(4,4), padding='SAME') #Out N/4 x N/4\n",
    "        h1 = tf.nn.relu(a1)\n",
    "        mp1 = tf.layers.max_pooling2d(h1, pool_size=(3,3), strides=(2,2), padding='SAME')    #Out N/2 x N/2\n",
    "        bn1 = tf.layers.batch_normalization(mp1, training=self.is_training)\n",
    "        \n",
    "        a2 = tf.layers.conv2d(bn1, filters=256, kernel_size=(5,5), strides=(1,1), padding='SAME')     #Out N x N\n",
    "        h2 = tf.nn.relu(a2)\n",
    "        mp2 = tf.layers.max_pooling2d(h2, pool_size=(3,3), strides=(2,2), padding='SAME')    #Out N/2 x N/2\n",
    "        bn2 = tf.layers.batch_normalization(mp2, training=self.is_training)              \n",
    "    \n",
    "        a3 = tf.layers.conv2d(bn2, filters=384, kernel_size=(3,3), strides=(1,1), padding='SAME')     #Out N x N\n",
    "        h3 = tf.nn.relu(a3)\n",
    "        a4 = tf.layers.conv2d(h3, filters=384, kernel_size=(3,3), strides=(1,1), padding='SAME')      #Out N x N\n",
    "        h4 = tf.nn.relu(a4)\n",
    "        a5 = tf.layers.conv2d(h4, filters=256, kernel_size=(3,3), strides=(1,1), padding='SAME')      #Out N x N\n",
    "        h5 = tf.nn.relu(a5)\n",
    "        mp3 = tf.layers.max_pooling2d(h5, pool_size=(3,3), strides=(2,2), padding='SAME')   #Out N/2 x N/2\n",
    "    \n",
    "        mp_flat = tf.reshape(mp3,[-1,4096])\n",
    "        aff1 = tf.layers.dense(mp_flat, 1792)\n",
    "        h6 = tf.nn.relu(aff1)\n",
    "        aff2 = tf.layers.dense(h6, 1792)\n",
    "        h7 = tf.nn.relu(aff2)\n",
    "        y_out = tf.layers.dense(h7, self.config.class_size)\n",
    "    \n",
    "        return y_out        \n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        target_vec = tf.one_hot(self.target, self.config.class_size)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=target_vec, logits=self.prediction)\n",
    "        #cross_entropy = tf.losses.hinge_loss(labels=target_vec, logits=self.prediction)\n",
    "        cross_entropy_sum = tf.reduce_sum(cross_entropy)\n",
    "        return cross_entropy_sum\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        #opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "\n",
    "        train_step = opt.minimize(self.cost)\n",
    "        \n",
    "        # batch normalization in tensorflow requires this extra dependency\n",
    "        # this is required to update the moving mean and moving variance variables\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            train_step = opt.minimize(self.cost)\n",
    "        \n",
    "        return(train_step)\n",
    "    \n",
    "    @lazy_property\n",
    "    def accuracy(self):\n",
    "        incorrect = tf.equal(tf.argmax(self.prediction, axis = 1), self.target)\n",
    "        return tf.reduce_mean( tf.cast(incorrect, tf.float32) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Size of Your Output as Sanity Check\n",
    "\n",
    "Input random numbers into X for feed dictionary and check the size of the output makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minibatch determining training error and accuracy to avoid RAM issues\n",
    "def train_batch(session, model, x_train, y_train, batch_size, print_batch = False):\n",
    "    # Loop over minibatches\n",
    "    cost = 0\n",
    "    correct = 0\n",
    "    sample_size = np.shape(x_train)[0]\n",
    "    for j,i in enumerate(np.arange(0, sample_size, batch_size)):\n",
    "        batch_x = x_train[i:i+batch_size,:,:,:]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        variables = [model.cost, model.accuracy]\n",
    "        cost_i, accuracy_i = session.run(variables, \\\n",
    "            {model.data:batch_x, model.target:batch_y, model.is_training:False})\n",
    "        num_sampled = np.shape(batch_x)[0]\n",
    "        cost += cost_i\n",
    "        correct += accuracy_i*num_sampled\n",
    "        if print_batch:\n",
    "            # print run time, current batch, and current epoch\n",
    "            print(\"Batch {:d}/{:d}\".format(j+1, int(round(sample_size/batch_size))) )\n",
    "            print(\"cost: \", cost_i)\n",
    "            print(\"accuracy: \", accuracy_i)\n",
    "            print(\"num_sampled: \", num_sampled)\n",
    "            print()\n",
    "    accuracy = np.float(correct/sample_size)\n",
    "    \n",
    "    return cost, accuracy \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now we're going to feed a random batch into the model \n",
    "# and make sure the output is the right size\n",
    "config = Config(sample_size=100, class_size=20, image_height=128, image_width=128, image_depth=3)\n",
    "x = np.random.randn(config.sample_size, config.image_height, config.image_width, config.image_depth)\n",
    "y = np.random.randn(config.sample_size)\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\"\n",
    "\n",
    "        model = AlexNet(config, learning_rate=0.003)\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        variables = [model.prediction, model.cost, model.accuracy]\n",
    "        ans_rnd, cost_rnd, acc_rnd = sess.run(variables,feed_dict={model.data:x, model.target:y, model.is_training:False})\n",
    "        cost, acc = train_batch(sess, model, X_train, y_train, batch_size = 2000, print_batch=True)\n",
    "        #ans, cost, acc = sess.run(variables,feed_dict={model.data:X_train, model.target:y_train, model.is_training:False})\n",
    "       \n",
    "        print('random data')\n",
    "        print('prediction shape: ',ans_rnd.shape)\n",
    "        print(np.array_equal(ans_rnd.shape, np.array([100, 20])))\n",
    "        print('cost: ', cost_rnd)\n",
    "        print('accuracy: ', acc_rnd)\n",
    "        print('\\ntraining data')\n",
    "        print('cost: ', cost)\n",
    "        print('accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data class for run\n",
    "class Data:\n",
    "    def __init__(self, X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for running model\n",
    "class Run:\n",
    "    \n",
    "    def __init__(self, session, config, data, model, epoch_size=15, minibatch_size = 100, \n",
    "                 learning_rate = 0.003, save_flag = False, print_every = 100, saver_address=''):\n",
    "        self.session = session \n",
    "        self.config = config\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.epoch_size = epoch_size\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.save_flag = save_flag\n",
    "        self.print_every = print_every\n",
    "        self.saver_address = saver_address\n",
    "        self.loader_address = loader_address\n",
    "        self._train_loss_hist = []\n",
    "        self._val_loss_hist = []\n",
    "        self._train_acc_hist = []\n",
    "        self._val_acc_hist = []\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # Save model parameters\n",
    "        if self.save_flag == True:    \n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        self.session.run( tf.global_variables_initializer() )\n",
    "        \n",
    "        # Loop over epochs\n",
    "        for epoch in range(self.epoch_size):\n",
    "            startTime_epoch = time.clock()\n",
    "            startTime_batch = time.clock()\n",
    "            \n",
    "            # Loop over minibatches\n",
    "            for j,i in enumerate(np.arange(0, self.config.sample_size, self.minibatch_size)):\n",
    "                batch_x = self.data.X_train[i:i+self.minibatch_size,:,:,:]\n",
    "                batch_y = self.data.Y_train[i:i+self.minibatch_size]\n",
    "                self.session.run(self.model.optimize, \n",
    "                                 {self.model.data:batch_x, self.model.target:batch_y, self.model.is_training:True})\n",
    "                \n",
    "                # print run time, current batch, and current epoch\n",
    "                if not j%self.print_every:\n",
    "                    batch_time = time.clock() - startTime_batch\n",
    "                    startTime_batch = time.clock()\n",
    "                    print(\"Batch {:d}/{:d} of epoch {:d} finished in {:f} seconds\".format(j+1,  \\\n",
    "                        int(round(self.config.sample_size/self.minibatch_size)), (epoch+1), batch_time))\n",
    "             \n",
    "            # Print current output, return losses, and return accuracies\n",
    "            epoch_time = time.clock() - startTime_epoch\n",
    "            print(\"Epoch {:d} finished in {:f} seconds\".format( (epoch + 1), epoch_time))\n",
    "            variables = [self.model.cost, self.model.accuracy]\n",
    "            loss_train, acc_train = train_batch(session, model, self.data.X_train, self.data.Y_train, batch_size = 2000)\n",
    "            loss_val, acc_val = self.session.run(variables, \\\n",
    "                {self.model.data:self.data.X_val, self.model.target:self.data.Y_val, self.model.is_training:False})\n",
    "            print('Epoch:{:2d}, Training Accuracy {:3.1f}%, Vallidation Accuracy:{:3.1f}%'.format( \\\n",
    "                (epoch + 1), (100*acc_train), (100*acc_val)))\n",
    "            \n",
    "            # Append losses and accuracies to list\n",
    "            self._train_loss_hist.append(loss_train)\n",
    "            self._val_loss_hist.append(loss_val)\n",
    "            self._train_acc_hist.append(acc_train)\n",
    "            self._val_acc_hist.append(acc_val)\n",
    "            \n",
    "        # Save model\n",
    "        if self.save_flag == True: \n",
    "            # Save trained model to data folder\n",
    "            saver.save(self.session, self.saver_address + 'classification_model')      \n",
    "            \n",
    "    def return_loss_acc(self):\n",
    "        return self._train_loss_hist, self._val_loss_hist, self._train_acc_hist, self._val_acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate Config instance\n",
    "config = Config(sample_size=np.shape(X_train)[0], class_size=20, image_height=128, image_width=128, image_depth=3)\n",
    "\n",
    "# Generate Data instance\n",
    "data = Data(X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "tf.reset_default_graph()\n",
    "model = AlexNet(config, learning_rate = 0.003)\n",
    "# Create run instance\n",
    "session = tf.Session()\n",
    "run = Run(session, config, data, model, epoch_size=15, learning_rate=0.003, print_every = 100, minibatch_size=100,\n",
    "         saver_address=r'/home/tylerchase/CS-231N-Final-Project/src/subreddit_classification_parameters/', \n",
    "         save_flag=True)\n",
    "run.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return Loss and Accuracy History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_hist, val_loss_hist, train_acc_hist, val_acc_hist = run.return_loss_acc()\n",
    "val_loss_hist_scale = np.array(val_loss_hist)/np.shape(X_val)[0]\n",
    "train_loss_hist_scale = np.array(train_loss_hist)/np.shape(X_train)[0]\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.plot(train_loss_hist_scale, label = 'train')\n",
    "ax1.plot(val_loss_hist_scale, label = 'val')\n",
    "\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.plot(train_acc_hist, label = 'train')\n",
    "ax2.plot(val_acc_hist, label = 'val')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Graph\n",
    "tf.reset_default_graph()\n",
    "loader_address=r'./subreddit_classification_parameters/classification_model'\n",
    "\n",
    "# Create model instance\n",
    "model = AlexNet(config, learning_rate = 0.003)\n",
    "\n",
    "# Load Saved Model\n",
    "session = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, loader_address) \n",
    "\n",
    "# Test Model Accuracy\n",
    "loss_train, acc_train = train_batch(session, model, data.X_train, data.Y_train, batch_size = 2000)\n",
    "variables = [model.cost, model.accuracy]\n",
    "loss_val, acc_val = session.run(variables, \\\n",
    "    {model.data:data.X_val, model.target:data.Y_val, model.is_training:False})\n",
    "\n",
    "print('Training Accuracy {:3.1f}%, Vallidation Accuracy:{:3.1f}%'.format((100*acc_train), (100*acc_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Predictions for Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val_pred = session.run(model.prediction, {model.data: data.X_val, model.target: data.Y_val, model.is_training:False})\n",
    "\n",
    "y_val_pred = np.argmax(y_val_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot Confusion Matrix for Subreddit Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          save_address = ''):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(11,11))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j],2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    #plt.savefig(save_address + 'confusion_mat.png')\n",
    "\n",
    "classes = [\"\"] * len(dictionary)\n",
    "for sub, ind in dictionary.items():\n",
    "    classes[ind] = sub\n",
    "\n",
    "conf = confusion_matrix(y_val, y_val_pred)\n",
    "plot_confusion_matrix(conf, classes=classes, normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
