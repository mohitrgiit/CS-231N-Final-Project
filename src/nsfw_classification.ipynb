{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone Model NSFW Classification\n",
    "\n",
    "This code uses a classification model to classify an image as either not safe for work (nsfw) or safe for work (not nsfw). For the sake of the milestone we implement an Alex Net model and analyze the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data and split into training validation and testing sets\n",
    "\n",
    "# Function for permuting and splitting data into training, developement, and test\n",
    "def import_dataset(address, file_names, train_percent = 80, dev_percent = 10):\n",
    "    SEED = 455\n",
    "    random.seed(SEED)\n",
    "    # Read csv file and create a list of tuples\n",
    "    with open(address + file_names['images']) as file:\n",
    "        images = pickle.load(file)\n",
    "    with open(address + file_names['nsfw']) as file_2:\n",
    "        subs = pickle.load(file_2)\n",
    "    with open(address + file_names['dict']) as file_3:\n",
    "        dictionary = pickle.load(file_3)\n",
    "    # Mix data and split into tran, dev, and test sets\n",
    "    N,W,H,C = np.shape(images)\n",
    "    indices = np.arange(N)\n",
    "    random.shuffle(indices)\n",
    "    images = images[indices]\n",
    "    subs = subs[indices]\n",
    "    #length = len(data)\n",
    "    train_end = int(train_percent*N/100)\n",
    "    dev_end = train_end + int(dev_percent*N/100)\n",
    "    X_train = images[:train_end]\n",
    "    y_train = subs[:train_end]\n",
    "    X_val = images[train_end:dev_end]\n",
    "    y_val = subs[train_end:dev_end]\n",
    "    X_test = images[dev_end:]\n",
    "    y_test = subs[dev_end:]\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Form training, developement, and testing data sets\n",
    "address = ''\n",
    "file_names = {}\n",
    "file_names['images'] = ''\n",
    "file_names['nsfw'] = ''\n",
    "file_names['dict'] = ''\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = import_dataset(address, file_names)\n",
    "\n",
    "# Print the sizes as a sanity check\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define AlexNet model \n",
    "\n",
    "* 11x11 convolutional layer with 96 filters and a stride of 4\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "* batch normalization\n",
    "\n",
    "\n",
    "* 5x5 convolutional layer with 256 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "* batch normalization\n",
    "\n",
    "\n",
    "* 3x3 convolutional layer with 384 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 convolutional layer with 384 filters and a stride of 1\n",
    "* ReLU activation \n",
    "* 3x3 convolutional layer with 256 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "\n",
    "\n",
    "* affine layer from 4096 to 1792\n",
    "* ReLU activation\n",
    "* affine layer from 1792 to 1792\n",
    "* ReLU activation\n",
    "* affine layer from 1792 to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 128, 128, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "Ncategories = 2\n",
    "\n",
    "def AlexNet(X, y, is_training):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # Convolutional Variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[11, 11, 3, 96])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[96])\n",
    "    Wconv2 = tf.get_variable(\"Wconv2\", shape=[5, 5, 96, 256])\n",
    "    bconv2 = tf.get_variable(\"bconv2\", shape=[256])\n",
    "    Wconv3 = tf.get_variable(\"Wconv3\", shape=[3, 3, 256, 384])\n",
    "    bconv3 = tf.get_variable(\"bconv3\", shape=[384])\n",
    "    Wconv4 = tf.get_variable(\"Wconv4\", shape=[3, 3, 384, 384])\n",
    "    bconv4 = tf.get_variable(\"bconv4\", shape=[384])\n",
    "    Wconv5 = tf.get_variable(\"Wconv5\", shape=[3, 3, 384, 256])\n",
    "    bconv5 = tf.get_variable(\"bconv5\", shape=[256])\n",
    "    \n",
    "    # Fully Connected Variables\n",
    "    W1 = tf.get_variable(\"W1\", shape=[4096, 1792])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[1792])\n",
    "    W2 = tf.get_variable(\"W2\", shape=[1792, 1792])\n",
    "    b2 = tf.get_variable(\"b2\", shape=[1792])\n",
    "    W3 = tf.get_variable(\"W3\", shape=[1792, Ncategories])\n",
    "    b3 = tf.get_variable(\"b3\", shape=[Ncategories])\n",
    "\n",
    "    # define our graph (e.g. AlexNet)\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,4,4,1], padding='SAME') + bconv1       #Out N/4 x N/4\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    mp1 = tf.nn.max_pool(h1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME')   #Out N/2 x N/2\n",
    "    bn1 = tf.layers.batch_normalization(mp1, training=is_training)\n",
    "        \n",
    "    a2 = tf.nn.conv2d(bn1, Wconv2, strides=[1,1,1,1], padding='SAME') + bconv2     #Out N x N\n",
    "    h2 = tf.nn.relu(a2)\n",
    "    mp2 = tf.nn.max_pool(h2, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME')   #Out N/2 x N/2\n",
    "    bn2 = tf.layers.batch_normalization(mp2, training=is_training)              \n",
    "    \n",
    "    a3 = tf.nn.conv2d(bn2, Wconv3, strides=[1,1,1,1], padding='SAME') + bconv3     #Out N x N\n",
    "    h3 = tf.nn.relu(a3)\n",
    "    a4 = tf.nn.conv2d(h3, Wconv4, strides=[1,1,1,1], padding='SAME') + bconv4      #Out N x N\n",
    "    h4 = tf.nn.relu(a3)\n",
    "    a5 = tf.nn.conv2d(h4, Wconv5, strides=[1,1,1,1], padding='SAME') + bconv5      #Out N x N\n",
    "    h5 = tf.nn.relu(a5)\n",
    "    mp3 = tf.nn.max_pool(h5, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME')   #Out N/2 x N/2\n",
    "    \n",
    "    mp_flat = tf.reshape(mp3,[-1,4096])\n",
    "    aff1 = tf.matmul(mp_flat, W1) + b1\n",
    "    h6 = tf.nn.relu(aff1)\n",
    "    aff2 = tf.matmul(h6, W2) + b2\n",
    "    h7 = tf.nn.relu(aff2)\n",
    "    y_out = tf.matmul(h7, W3) + b3\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "# Define y_out in graph\n",
    "y_out = AlexNet(X, y, is_training)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,Ncategories),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Size of Your Output as Sanity Check\n",
    "\n",
    "Input random numbers into X for feed dictionary and check the size of the output makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we're going to feed a random batch into the model \n",
    "# and make sure the output is the right size\n",
    "x = np.random.randn(64, 128, 128,3)\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\"\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        ans = sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "        print(ans.shape)\n",
    "        print(np.array_equal(ans.shape, np.array([64, 20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the model returning total loss and total correct\n",
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,10,64,100,train_step,True)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model on the Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model on the Test Data\n",
    "\n",
    "This is done only once after determining hyperparameters on the developement set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
