{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Tyler Chase\n",
    "\n",
    "Date: 2017/05/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone Model Subreddit Classification\n",
    "\n",
    "This code uses a classification model to classify an image to one of 20 subreddits. For the sake of the milestone we implement an Alex Net model and analyze the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function for building graph\n",
    "\n",
    "This function reduces the text needed for running the @property wrapper making the code more readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduces the text needed for running @property making code more readable\n",
    "def lazy_property(function):\n",
    "    # Attribute used to test if code chunk has been run or not\n",
    "    attribute = '_lazy_' + function.__name__\n",
    "    # run wrapper function when wrapper returned below\n",
    "    @property\n",
    "    # Keeps original function attributes such as function.__name__ \n",
    "    # Otherwise it would be replaced with the wrapper attributes\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        # If doesn't have (attribute) then code chunk hasn't been run\n",
    "        if not hasattr(self, attribute):\n",
    "            # Run code chunk and store it in (attribute) of class\n",
    "            setattr(self, attribute, function(self))     \n",
    "        # return the value of the number stored in (attribute)\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import functools\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and split into training validation and testing sets\n",
    "\n",
    "# Function for permuting and splitting data into training, developement, and test\n",
    "def import_dataset(address, file_names, train_percent = 80, dev_percent = 10):\n",
    "    SEED = 455\n",
    "    random.seed(SEED)\n",
    "    # Read csv file and create a list of tuples\n",
    "    images = np.load(address+file_names['images'])\n",
    "    images = images.astype(float)\n",
    "    with open(address + file_names['subs'], 'rb') as file_2:\n",
    "        subs = pickle.load(file_2)\n",
    "        subs = np.array(subs)\n",
    "    with open(address + file_names['dict'], 'rb') as file_3:\n",
    "        dictionary = pickle.load(file_3)\n",
    "    # Mix data and split into tran, dev, and test sets\n",
    "    N,W,H,C = np.shape(images)\n",
    "    indices = np.arange(N)\n",
    "    random.shuffle(indices)\n",
    "    images = images[indices]\n",
    "    subs = subs[indices]\n",
    "    train_end = int(train_percent*N/100)\n",
    "    dev_end = train_end + int(dev_percent*N/100)\n",
    "    X_train = images[:train_end]\n",
    "    y_train = subs[:train_end]\n",
    "    X_val = images[train_end:dev_end]\n",
    "    y_val = subs[train_end:dev_end]\n",
    "    X_test = images[dev_end:]\n",
    "    y_test = subs[dev_end:]\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, dictionary\n",
    "\n",
    "# Form training, developement, and testing data sets\n",
    "address = r'/Users/tylerchase/Documents/Stanford_Classes/CS231n_CNN_for_Visual_Recognition/final_project/milestoneData/Archive//'\n",
    "file_names = {}\n",
    "file_names['images'] = 'final_output_data.npy'\n",
    "file_names['subs'] = 'final_output_labels'\n",
    "file_names['dict'] = 'final_output_subredditIndex'\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, dictionary = import_dataset(address, file_names)\n",
    "\n",
    "# Print the sizes as a sanity check\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define AlexNet model \n",
    "\n",
    "* 11x11 convolutional layer with 96 filters and a stride of 4\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "* batch normalization\n",
    "\n",
    "\n",
    "* 5x5 convolutional layer with 256 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "* batch normalization\n",
    "\n",
    "\n",
    "* 3x3 convolutional layer with 384 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 convolutional layer with 384 filters and a stride of 1\n",
    "* ReLU activation \n",
    "* 3x3 convolutional layer with 256 filters and a stride of 1\n",
    "* ReLU activation\n",
    "* 3x3 max pooling with a stride of 2\n",
    "\n",
    "\n",
    "* affine layer from 4096 to 1792\n",
    "* ReLU activation\n",
    "* affine layer from 1792 to 1792\n",
    "* ReLU activation\n",
    "* affine layer from 1792 to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration file storing important values\n",
    "class Config:\n",
    "    def __init__(self, sample_size, class_size, image_width, image_height, image_depth):\n",
    "        self.sample_size = sample_size\n",
    "        self.class_size = class_size\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "        self.image_depth = image_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet:\n",
    "    \n",
    "    def __init__(self, config, learning_rate=0.003):\n",
    "        self.config = config\n",
    "        self.data, self.target, self.is_training = self._initialize_placeholders()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.prediction\n",
    "        self.cost\n",
    "        self.optimize\n",
    "        self.accuracy\n",
    "        \n",
    "    def _initialize_placeholders(self):\n",
    "        data = tf.placeholder(tf.float32, [None, self.config.image_height, \n",
    "                                         self.config.image_width, self.config.image_depth])\n",
    "        target = tf.placeholder(tf.int64, [None])\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "        return(data, target, is_training)\n",
    "        \n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # define our graph (e.g. AlexNet)\n",
    "        \n",
    "        a1 = tf.layers.conv2d(self.data, filters=96, kernel_size=(11,11), strides=(4,4), padding='SAME') #Out N/4 x N/4\n",
    "        h1 = tf.nn.relu(a1)\n",
    "        mp1 = tf.layers.max_pooling2d(h1, pool_size=(3,3), strides=(2,2), padding='SAME')    #Out N/2 x N/2\n",
    "        bn1 = tf.layers.batch_normalization(mp1, training=self.is_training)\n",
    "        \n",
    "        a2 = tf.layers.conv2d(bn1, filters=256, kernel_size=(5,5), strides=(1,1), padding='SAME')     #Out N x N\n",
    "        h2 = tf.nn.relu(a2)\n",
    "        mp2 = tf.layers.max_pooling2d(h2, pool_size=(3,3), strides=(2,2), padding='SAME')    #Out N/2 x N/2\n",
    "        bn2 = tf.layers.batch_normalization(mp2, training=self.is_training)              \n",
    "    \n",
    "        a3 = tf.layers.conv2d(bn2, filters=384, kernel_size=(3,3), strides=(1,1), padding='SAME')     #Out N x N\n",
    "        h3 = tf.nn.relu(a3)\n",
    "        a4 = tf.layers.conv2d(h3, filters=384, kernel_size=(3,3), strides=(1,1), padding='SAME')      #Out N x N\n",
    "        h4 = tf.nn.relu(a4)\n",
    "        a5 = tf.layers.conv2d(h4, filters=256, kernel_size=(3,3), strides=(1,1), padding='SAME')      #Out N x N\n",
    "        h5 = tf.nn.relu(a5)\n",
    "        mp3 = tf.layers.max_pooling2d(h5, pool_size=(3,3), strides=(2,2), padding='SAME')   #Out N/2 x N/2\n",
    "    \n",
    "        mp_flat = tf.reshape(mp3,[-1,4096])\n",
    "        aff1 = tf.layers.dense(mp_flat, 1792)\n",
    "        h6 = tf.nn.relu(aff1)\n",
    "        aff2 = tf.layers.dense(h6, 1792)\n",
    "        h7 = tf.nn.relu(aff2)\n",
    "        y_out = tf.layers.dense(h7, self.config.class_size)\n",
    "    \n",
    "        return y_out        \n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        target_vec = tf.one_hot(self.target, self.config.class_size)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=target_vec, logits=self.prediction)\n",
    "        #cross_entropy = tf.losses.hinge_loss(labels=target_vec, logits=self.prediction)\n",
    "        cross_entropy_sum = tf.reduce_sum(cross_entropy)\n",
    "        return cross_entropy_sum\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        #opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "\n",
    "        train_step = opt.minimize(self.cost)\n",
    "        \n",
    "        # batch normalization in tensorflow requires this extra dependency\n",
    "        # this is required to update the moving mean and moving variance variables\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            train_step = opt.minimize(self.cost)\n",
    "        \n",
    "        return(train_step)\n",
    "    \n",
    "    @lazy_property\n",
    "    def accuracy(self):\n",
    "        incorrect = tf.equal(tf.argmax(self.prediction, axis = 1), self.target)\n",
    "        return tf.reduce_mean( tf.cast(incorrect, tf.float32) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Size of Your Output as Sanity Check\n",
    "\n",
    "Input random numbers into X for feed dictionary and check the size of the output makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now we're going to feed a random batch into the model \n",
    "# and make sure the output is the right size\n",
    "config = Config(sample_size=100, class_size=20, image_height=128, image_width=128, image_depth=3)\n",
    "x = np.random.randn(config.sample_size, config.image_height, config.image_width, config.image_depth)\n",
    "y = np.random.randn(config.sample_size)\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\"\n",
    "\n",
    "        model = AlexNet(config, learning_rate=0.003)\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        variables = [model.prediction, model.cost, model.accuracy]\n",
    "        ans_rnd, cost_rnd, acc_rnd = sess.run(variables,feed_dict={model.data:x, model.target:y, model.is_training:True})\n",
    "        ans, cost, acc = sess.run(variables,feed_dict={model.data:X_val, model.target:y_val, model.is_training:True})\n",
    "       \n",
    "        print('random data')\n",
    "        print('prediction shape: ',ans_rnd.shape)\n",
    "        print(np.array_equal(ans_rnd.shape, np.array([100, 20])))\n",
    "        print('cost: ', cost_rnd)\n",
    "        print('accuracy: ', acc_rnd)\n",
    "        print('\\nvalidation data')\n",
    "        print('prediction shape: ',ans.shape)\n",
    "        print('cost: ', cost)\n",
    "        print('accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data class for run\n",
    "class Data:\n",
    "    def __init__(self, X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for running model\n",
    "class Run:\n",
    "    \n",
    "    def __init__(self, session, config, data, model, epoch_size=15, minibatch_size = 100, \n",
    "                 learning_rate = 0.003, save_flag = False, print_every = 100, saver_address='',\n",
    "                 loader_address=''):\n",
    "        self.session = session \n",
    "        self.config = config\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.epoch_size = epoch_size\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.save_flag = save_flag\n",
    "        self.print_every = print_every\n",
    "        self.saver_address = saver_address\n",
    "        self.loader_address = loader_address\n",
    "        self._train_loss_hist = []\n",
    "        self._val_loss_hist = []\n",
    "        self._train_acc_hist = []\n",
    "        self._val_acc_hist = []\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # Save model parameters\n",
    "        if self.save_flag == True:    \n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        self.session.run( tf.global_variables_initializer() )\n",
    "        \n",
    "        # Loop over epochs\n",
    "        for epoch in range(self.epoch_size):\n",
    "            startTime_epoch = time.clock()\n",
    "            startTime_batch = time.clock()\n",
    "            \n",
    "            # Loop over minibatches\n",
    "            for j,i in enumerate(np.arange(0, self.config.sample_size, self.minibatch_size)):\n",
    "                batch_x = self.data.X_train[i:i+self.minibatch_size,:,:,:]\n",
    "                batch_y = self.data.Y_train[i:i+self.minibatch_size]\n",
    "                self.session.run(self.model.optimize, \n",
    "                                 {self.model.data:batch_x, self.model.target:batch_y, self.model.is_training:True})\n",
    "                \n",
    "                # print run time, current batch, and current epoch\n",
    "                if not j%self.print_every:\n",
    "                    batch_time = time.clock() - startTime_batch\n",
    "                    startTime_batch = time.clock()\n",
    "                    print(\"Batch {:d}/{:d} of epoch {:d} finished in {:f} seconds\".format(j+1,  \\\n",
    "                        int(self.config.sample_size/self.minibatch_size), (epoch+1), batch_time))\n",
    "             \n",
    "            # Print current output, return losses, and return accuracies\n",
    "            epoch_time = time.clock() - startTime_epoch\n",
    "            print(\"Epoch {:d} finished in {:f} seconds\".format( (epoch + 1), epoch_time))\n",
    "            variables = [self.model.cost, self.model.accuracy]\n",
    "            loss_train, acc_train = self.session.run(variables, \\\n",
    "                {self.model.data:self.data.X_train, self.model.target:self.data.Y_train, self.model.is_training:False})\n",
    "            loss_val, acc_val = self.session.run(variables, \\\n",
    "                {self.model.data:self.data.X_val, self.model.target:self.data.Y_val, self.model.is_training:False})\n",
    "            print('Epoch:{:2d}, Training Accuracy {:3.1f}%, Test Accuracy:{:3.1f}%'.format( \\\n",
    "                (epoch + 1), (100*acc_train), (100*acc_test)))\n",
    "            \n",
    "            # Append losses and accuracies to list\n",
    "            self._train_loss_hist.append(loss_train)\n",
    "            self._val_loss_hist.append(loss_val)\n",
    "            self._train_acc_hist.append(acc_train)\n",
    "            self._val_acc_hist.append(acc_val)\n",
    "            \n",
    "            # Save model\n",
    "            if self.save_flag == True: \n",
    "                # Save trained model to data folder\n",
    "                saver.save(sess, self.saver_address + 'classification_model')    \n",
    "                \n",
    "    def load(self):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.session, self.loader_address)   \n",
    "            \n",
    "    def return_loss_acc(self):\n",
    "        return self._train_loss_hist, self._val_loss_hist, self._train_acc_hist, self._val_acc_hist\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate Config instance\n",
    "config = Config(sample_size=np.shape(X_val)[0], class_size=20, image_height=128, image_width=128, image_depth=3)\n",
    "\n",
    "# Generate Data instance\n",
    "data = Data(X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "tf.reset_default_graph()\n",
    "model = AlexNet(config, learning_rate = 0.003)\n",
    "# Create run instance\n",
    "session = tf.Session()\n",
    "run = Run(session, config, data, model, epoch_size=1, learning_rate=0.003, print_every = 1, minibatch_size=10)\n",
    "run.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model on the Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Predictions for Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val_pred = sess.run(y_out, {X: X_val, y: y_val, is_training: 0})\n",
    "\n",
    "y_val_pred = np.argmax(y_val_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot Confusion Matrix for Subreddit Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          save_address = ''):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(11,11))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j],2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    #plt.savefig(save_address + 'confusion_mat.png')\n",
    "\n",
    "classes = [\"\"] * len(dictionary)\n",
    "for sub, ind in dictionary.items():\n",
    "    classes[ind] = sub\n",
    "\n",
    "conf = confusion_matrix(y_val, y_val_pred)\n",
    "plot_confusion_matrix(conf, classes=classes, normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
